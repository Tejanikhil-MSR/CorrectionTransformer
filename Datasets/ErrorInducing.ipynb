{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ced1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import LookUpTable as lut\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62749ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SamantarDatasetPath = r'./SamantarDatasetRaw.txt'\n",
    "webscrapedDatasetPath = r'./web_scraped_data_raw.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd8b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_telugu_text(text):\n",
    "    # Remove non-Telugu characters except whitespace\n",
    "    cleaned = re.sub(r'[^\\u0C00-\\u0C7F\\s]', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    # Strip unnecessary leading/trailing whitespace\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddbd42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levenstein(src, tar):\n",
    "    return Levenshtein.distance(src, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08df5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_consonants(text, error_percentage):\n",
    "    characters = list(text)\n",
    "    max_errors = int(len(characters) * error_percentage)\n",
    "    error_count = 0\n",
    "    tried_indices = set()\n",
    "    \n",
    "    start_length = len(characters)\n",
    "\n",
    "    while error_count < max_errors and len(tried_indices) < len(characters):\n",
    "        idx = random.randint(0, len(characters) - 1)\n",
    "        if idx in tried_indices:\n",
    "            continue\n",
    "        tried_indices.add(idx)\n",
    "\n",
    "        current_char = characters[idx]\n",
    "\n",
    "        # Skip if character is not a consonant present the lookup table\n",
    "        if current_char not in lut.SimpleCharacterReplacements.keys():\n",
    "            continue\n",
    "\n",
    "        # Skip if previous character is a virama (్)\n",
    "        if (idx > 0 and characters[idx - 1] == '్') or (characters[idx]==\"య\" and idx < len(characters) - 1 and characters[idx + 1] == \"ి\"):\n",
    "            continue\n",
    "\n",
    "        # Get replacement options and choose one randomly\n",
    "        replacements = lut.SimpleCharacterReplacements[current_char]\n",
    "        replacement = random.choice(replacements)\n",
    "\n",
    "        characters[idx] = replacement\n",
    "        error_count += 1\n",
    "\n",
    "    end_length = len(characters)\n",
    "\n",
    "    avg_error = 0\n",
    "    for (actual, error) in zip(text.split(\" \"), ''.join(characters).split(\" \")):\n",
    "        avg_error += get_levenstein(actual, error)\n",
    "        \n",
    "    AverageErrorPerWord = round(avg_error / len(text.split(\" \")), 2)\n",
    "    PercentageErrorInduced = round((error_count / start_length), 2) *100\n",
    "\n",
    "    return ''.join(characters), PercentageErrorInduced, AverageErrorPerWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5172ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_stacked_diacritics(text, error_percentage):\n",
    "    \n",
    "    telugu_consonants = ['క', 'ఖ', 'గ', 'ఘ', 'ఙ','చ', 'ఛ', 'జ', 'ఝ', 'ఞ',\n",
    "                         'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న',\n",
    "                        'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ల', 'వ',\n",
    "                        'శ', 'ష', 'స', 'హ', 'ళ', 'క్ష', 'ఱ']\n",
    "\n",
    "    characters = list(text)\n",
    "    max_errors = int(len(characters) * error_percentage)\n",
    "    error_count = 0\n",
    "    tried_indices = set()\n",
    "    \n",
    "    start_length = len(characters)\n",
    "\n",
    "    while error_count < max_errors and len(tried_indices) < len(characters):\n",
    "        idx = random.randint(0, len(characters) - 1)\n",
    "        if idx in tried_indices:\n",
    "            continue\n",
    "        tried_indices.add(idx)\n",
    "\n",
    "        current_char = characters[idx]\n",
    "\n",
    "        # Skip if character is not a consonant present the lookup table\n",
    "        if current_char not in lut.StackingReplacements.keys() and current_char not in lut.Diacritic_replacements.keys():\n",
    "            continue\n",
    "\n",
    "        # Check if previous character is a virama (్) only then we can replace the current character\n",
    "        if ((idx > 2 and characters[idx - 1] == '్' and characters[idx - 2] in telugu_consonants) or\n",
    "            (current_char in lut.Diacritic_replacements.keys() and characters[idx - 1] in telugu_consonants)):\n",
    "            # Get replacement options and choose one randomly\n",
    "            if(current_char in lut.Diacritic_replacements.keys()):\n",
    "                replacements = lut.Diacritic_replacements[current_char]\n",
    "            else:\n",
    "                replacements = lut.StackingReplacements[current_char]\n",
    "    \n",
    "            replacement = random.choice(replacements)\n",
    "\n",
    "            characters[idx] = replacement\n",
    "            error_count += 1\n",
    "\n",
    "    avg_error = 0\n",
    "    for (actual, error) in zip(text.split(\" \"), ''.join(characters).split(\" \")):\n",
    "        avg_error += get_levenstein(actual, error)\n",
    "        \n",
    "    AverageErrorPerWord = round(avg_error / len(text.split(\" \")), 2)\n",
    "    PercentageErrorInduced = round((error_count / start_length), 2)\n",
    "\n",
    "    return ''.join(characters), PercentageErrorInduced, AverageErrorPerWord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae4f24",
   "metadata": {},
   "source": [
    "### Dealing with Samantar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a37d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SamantarDatasetPath, 'r', encoding='utf-8') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "df = pd.DataFrame(lines, columns=[\"OriginalText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7ccb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"OriginalText\"] = df[\"OriginalText\"].apply(clean_telugu_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c07fabd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text :  మమతా బెనర్జీ ఫైల్ ఇమేజ్\n",
      "['మ', 'మ', 'త', 'ా', ' ', 'బ', 'ె', 'న', 'ర', '్', 'జ', 'ీ', ' ', 'ఫ', 'ై', 'ల', '్', ' ', 'ఇ', 'మ', 'ే', 'జ', '్']\n",
      "Error Induced Text :  యమతా భెనర్జీ ఫైల్ ఇమేజ్\n",
      "Average Error per word :  9.0\n"
     ]
    }
   ],
   "source": [
    "random_text = df.sample(1).iloc[0]['OriginalText']\n",
    "print(\"Original Text : \", random_text)\n",
    "print(list(random_text))\n",
    "print(\"Error Induced Text : \", substitute_consonants(random_text, 0.1)[0])\n",
    "print(\"Average Error per word : \", substitute_consonants(random_text, 0.1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81ec80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[\"15%_ErrorInducedText\", \"Percentage_ErrorInduced_15%\", \"AverageErrorPerWord_15%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.15)).apply(pd.Series)\n",
    "# df[[\"25%_ErrorInducedText\", \"Percentage_ErrorInduced_25%\", \"AverageErrorPerWord_25%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.25)).apply(pd.Series)\n",
    "# df[[\"35%_ErrorInducedText\", \"Percentage_ErrorInduced_35%\", \"AverageErrorPerWord_35%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.35)).apply(pd.Series)\n",
    "# df[[\"50%_ErrorInducedText\", \"Percentage_ErrorInduced_50%\", \"AverageErrorPerWord_50%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.5)).apply(pd.Series)\n",
    "# df.to_csv('./SamantarDatasetWithDirectConsonantSubstitutions.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80847824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"15%_ErrorInducedText\", \"Percentage_ErrorInduced_15%\", \"AverageErrorPerWord_15%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.15)).apply(pd.Series)\n",
    "df[[\"25%_ErrorInducedText\", \"Percentage_ErrorInduced_25%\", \"AverageErrorPerWord_25%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.25)).apply(pd.Series)\n",
    "df[[\"35%_ErrorInducedText\", \"Percentage_ErrorInduced_35%\", \"AverageErrorPerWord_35%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.35)).apply(pd.Series)\n",
    "df[[\"50%_ErrorInducedText\", \"Percentage_ErrorInduced_50%\", \"AverageErrorPerWord_50%\"]] = df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.5)).apply(pd.Series)\n",
    "df.to_csv('./SamantarDatasetWithStackedSubstitutions.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea1b25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model needs to learn from more context and accordingly correct the text\n",
    "# substitute_stacked_diacritics(\"శంకరాచార్యులు అద్వైత సిద్ధాంతాన్ని ప్రవేశపెట్టారు\", 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633742e",
   "metadata": {},
   "source": [
    "### Dealing with Web Scraped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08ee9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df = pd.read_csv(webscrapedDatasetPath, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b57d824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum word count in the dataset: 500\n",
      "Min word count in the dataset: 13\n",
      "Number of outliers: 0\n"
     ]
    }
   ],
   "source": [
    "Q1 = scraped_df['word_count'].quantile(0.25)\n",
    "Q3 = scraped_df['word_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = scraped_df[(scraped_df['word_count'] < lower_bound) | (scraped_df['word_count'] > upper_bound)]\n",
    "\n",
    "print(\"Maximum word count in the dataset:\", scraped_df['word_count'].max())\n",
    "print(\"Min word count in the dataset:\", scraped_df['word_count'].min())\n",
    "\n",
    "print(\"Number of outliers:\", len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e51eaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are no outliers, split each sentence into 3 sentences of length total number of words/3 = 500/3 ~ 160\n",
    "max_sentence_length = 100 # max number of words in a sentence\n",
    "split_sentences_list = []\n",
    "for index, row in scraped_df.iterrows():\n",
    "    text = row['content']\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_sentence_length):\n",
    "        new_sentence = ' '.join(words[i:i + max_sentence_length])\n",
    "        split_sentences_list.append(new_sentence)\n",
    "\n",
    "# remove the empty strings from the list\n",
    "split_sentences_list = [sentence for sentence in split_sentences_list if sentence.strip() != '']\n",
    "\n",
    "# filter out the sentence with sentence length < 20 words\n",
    "split_sentences_list = [sentence for sentence in split_sentences_list if len(sentence.split()) >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b3cbba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences after splitting: 3240\n",
      "Max sentence length after splitting: 100\n",
      "Min sentence length after splitting: 20\n",
      "Average sentence length after splitting: 89.22901234567901\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences after splitting:\", len(split_sentences_list))\n",
    "print(\"Max sentence length after splitting:\", max([len(sentence.split()) for sentence in split_sentences_list]))\n",
    "print(\"Min sentence length after splitting:\", min([len(sentence.split()) for sentence in split_sentences_list]))\n",
    "print(\"Average sentence length after splitting:\", sum([len(sentence.split()) for sentence in split_sentences_list]) / len(split_sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf80165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_df = pd.DataFrame(split_sentences_list, columns=[\"OriginalText\"])\n",
    "scraped_data_df[\"OriginalText\"] = scraped_data_df[\"OriginalText\"].apply(clean_telugu_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a865661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraped_data_df[[\"15%_ErrorInducedText\", \"Percentage_ErrorInduced_15%\", \"AverageErrorPerWord_15%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.15)).apply(pd.Series)\n",
    "# scraped_data_df[[\"25%_ErrorInducedText\", \"Percentage_ErrorInduced_25%\", \"AverageErrorPerWord_25%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.25)).apply(pd.Series)\n",
    "# scraped_data_df[[\"35%_ErrorInducedText\", \"Percentage_ErrorInduced_35%\", \"AverageErrorPerWord_35%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.35)).apply(pd.Series)\n",
    "# scraped_data_df[[\"50%_ErrorInducedText\", \"Percentage_ErrorInduced_50%\", \"AverageErrorPerWord_50%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_consonants(x, 0.5)).apply(pd.Series)\n",
    "# scraped_data_df.to_csv('./WebScrapedDatasetWithDirectConsonantSubstitutions.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d48cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_df[[\"15%_ErrorInducedText\", \"Percentage_ErrorInduced_15%\", \"AverageErrorPerWord_15%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.15)).apply(pd.Series)\n",
    "scraped_data_df[[\"25%_ErrorInducedText\", \"Percentage_ErrorInduced_25%\", \"AverageErrorPerWord_25%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.25)).apply(pd.Series)\n",
    "scraped_data_df[[\"35%_ErrorInducedText\", \"Percentage_ErrorInduced_35%\", \"AverageErrorPerWord_35%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.35)).apply(pd.Series)\n",
    "scraped_data_df[[\"50%_ErrorInducedText\", \"Percentage_ErrorInduced_50%\", \"AverageErrorPerWord_50%\"]] = scraped_data_df[\"OriginalText\"].apply(lambda x: substitute_stacked_diacritics(x, 0.5)).apply(pd.Series)\n",
    "scraped_data_df.to_csv('./WebScrapedDatasetWithStackedSubstitutions.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef9693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audiofy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
