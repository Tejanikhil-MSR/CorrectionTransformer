{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c9784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\", src_lang=\"te_IN\", use_fast=False)\n",
    "\n",
    "# vocab = tokenizer.get_vocab()\n",
    "# telugu_tokens = [tok for tok in vocab.keys() if any('\\u0C00' <= ch <= '\\u0C7F' for ch in tok)]\n",
    "\n",
    "# print(telugu_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b189daa",
   "metadata": {},
   "source": [
    "The IndicBART tokenizer is based on SentencePiece, but instead of standard subword units like BPE or WordPiece, it's trained at the Unicode character level, where each token is often a:\n",
    "- Standalone consonant (క, గ, త, etc.)\n",
    "- Vowel sign or diacritic (ా, ి, ీ, etc.)\n",
    "- Word boundary marker (▁ for whitespace)\n",
    "\n",
    "This is intentional for Indic scripts because:\n",
    "\n",
    "- Indic languages are highly agglutinative, and subword segmentation can be noisy.\n",
    "- It's better to model individual aksharas (syllables) or character+diacritic units instead of full words or arbitrary subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973cebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5960193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1507fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3abeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbfe54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/mt5-small\"  # or any other model name you want to use\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name) # loads the tokenizer from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c32d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OriginalText', '15%_ErrorInducedText', 'Percentage_ErrorInduced_15%',\n",
      "       'AverageErrorPerWord_15%', '25%_ErrorInducedText',\n",
      "       'Percentage_ErrorInduced_25%', 'AverageErrorPerWord_25%',\n",
      "       '35%_ErrorInducedText', 'Percentage_ErrorInduced_35%',\n",
      "       'AverageErrorPerWord_35%', '50%_ErrorInducedText',\n",
      "       'Percentage_ErrorInduced_50%', 'AverageErrorPerWord_50%'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "SamantarData_DirCS = pd.read_csv(r\"./Datasets/SamantarDatasetWithDirectConsonantSubstitutions.csv\") # pure consonant replacement\n",
    "SamantarData_DiaS = pd.read_csv(r\"./Datasets/SamantarDatasetWithStackedSubstitutions.csv\") # Diacritic substitution errors\n",
    "WebScrapedData_DirCS = pd.read_csv(r\"./Datasets/SamantarDatasetWithDirectConsonantSubstitutions.csv\") # pure consonant replacement\n",
    "WebScrapedData_DiaS = pd.read_csv(r\"./Datasets/SamantarDatasetWithStackedSubstitutions.csv\") # Diacritic substitution errors\n",
    "print(SamantarData_DiaS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4cf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_percentages = ['15', '25', '35', '50']\n",
    "\n",
    "SamantarData_DirCS_processed = {}\n",
    "SamantarData_DiaS_processed = {}\n",
    "WebScrapedData_DirCS_processed = {}\n",
    "WebScrapedData_DiaS_processed = {}\n",
    "\n",
    "# Filter out the row sentences with more than 2 errors in the average error per word\n",
    "# and create a new DataFrame with the required columns for each error percentage\n",
    "for perc in error_percentages:\n",
    "\n",
    "    avg_col = f'AverageErrorPerWord_{perc}%'\n",
    "\n",
    "    df_Samantar = SamantarData_DirCS[(SamantarData_DirCS[avg_col] > 2)][['OriginalText', f'{perc}%_ErrorInducedText']].rename(columns={'OriginalText': 'input', f'{perc}%_ErrorInducedText': 'target'})\n",
    "    \n",
    "    SamantarData_DirCS_processed[f'SamantarData_DirCS_{perc}p'] = df_Samantar.to_dict(orient='records')\n",
    "    \n",
    "    df_webscraped = WebScrapedData_DirCS[(WebScrapedData_DirCS[avg_col] > 2)][['OriginalText', f'{perc}%_ErrorInducedText']].rename(columns={'OriginalText': 'input', f'{perc}%_ErrorInducedText': 'target'})\n",
    "\n",
    "    WebScrapedData_DirCS_processed[f'WebScrapedData_DirCS_{perc}p'] = df_webscraped.to_dict(orient='records')\n",
    "\n",
    "# Process DiaS\n",
    "for perc in error_percentages:\n",
    "    \n",
    "    avg_col = f'AverageErrorPerWord_{perc}%'\n",
    "\n",
    "    df_Samantar = SamantarData_DiaS[(SamantarData_DiaS[avg_col] > 2)][['OriginalText', f'{perc}%_ErrorInducedText']].rename(columns={'OriginalText': 'input', f'{perc}%_ErrorInducedText': 'target'})\n",
    "\n",
    "    SamantarData_DiaS_processed[f'SamantarData_DiaS_{perc}p'] = df_Samantar.to_dict(orient='records')\n",
    "    \n",
    "    df_webscraped = WebScrapedData_DiaS[(WebScrapedData_DiaS[avg_col] > 2)][['OriginalText', f'{perc}%_ErrorInducedText']].rename(columns={'OriginalText': 'input', f'{perc}%_ErrorInducedText': 'target'})\n",
    "\n",
    "    WebScrapedData_DiaS_processed[f'WebScrapedData_DiaS_{perc}p'] = df_webscraped.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf284ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    input_enc = tokenizer(text[\"input\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    target_enc = tokenizer(text[\"target\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61b6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"SamantarData_DirCS_15p\": SamantarData_DirCS_processed['SamantarData_DirCS_15p'],\n",
    "    \"SamantarData_DirCS_25p\": SamantarData_DirCS_processed['SamantarData_DirCS_25p'],\n",
    "    \"SamantarData_DirCS_35p\": SamantarData_DirCS_processed['SamantarData_DirCS_35p'],\n",
    "    \"SamantarData_DirCS_50p\": SamantarData_DirCS_processed['SamantarData_DirCS_50p'],\n",
    "    \n",
    "    \"SamantarData_DiaS_15p\": SamantarData_DiaS_processed['SamantarData_DiaS_15p'],\n",
    "    \"SamantarData_DiaS_25p\": SamantarData_DiaS_processed['SamantarData_DiaS_25p'],\n",
    "    \"SamantarData_DiaS_35p\": SamantarData_DiaS_processed['SamantarData_DiaS_35p'],\n",
    "    \"SamantarData_DiaS_50p\": SamantarData_DiaS_processed['SamantarData_DiaS_50p'],\n",
    "    \n",
    "    \"WebScrapedData_DirCS_15p\": WebScrapedData_DirCS_processed['WebScrapedData_DirCS_15p'],\n",
    "    \"WebScrapedData_DirCS_25p\": WebScrapedData_DirCS_processed['WebScrapedData_DirCS_25p'],\n",
    "    \"WebScrapedData_DirCS_35p\": WebScrapedData_DirCS_processed['WebScrapedData_DirCS_35p'],\n",
    "    \"WebScrapedData_DirCS_50p\": WebScrapedData_DirCS_processed['WebScrapedData_DirCS_50p'],\n",
    "    \n",
    "    \"WebScrapedData_DiaS_15p\": WebScrapedData_DiaS_processed['WebScrapedData_DiaS_15p'],\n",
    "    \"WebScrapedData_DiaS_25p\": WebScrapedData_DiaS_processed['WebScrapedData_DiaS_25p'],\n",
    "    \"WebScrapedData_DiaS_35p\": WebScrapedData_DiaS_processed['WebScrapedData_DiaS_35p'],\n",
    "    \"WebScrapedData_DiaS_50p\": WebScrapedData_DiaS_processed['WebScrapedData_DiaS_50p'],\n",
    "}\n",
    "\n",
    "tokenized_datasets = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    tokenized_datasets[f\"tokenized_{name}\"] = [preprocess(item) for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e9231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tokenized embedding :  torch.Size([1, 512])\n",
      "Shape of tokenized embedding :  torch.Size([1, 512])\n",
      "Shape of tokenized embedding :  torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of tokenized embedding : \", tokenized_datasets[\"tokenized_SamantarData_DirCS_15p\"][0][\"attention_mask\"].shape)\n",
    "print(\"Shape of tokenized embedding : \", tokenized_datasets[\"tokenized_SamantarData_DiaS_15p\"][0][\"attention_mask\"].shape)\n",
    "print(\"Shape of tokenized embedding : \", tokenized_datasets[\"tokenized_WebScrapedData_DirCS_15p\"][0][\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86d9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.squeeze() for k, v in self.data[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2335f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loaders = {}\n",
    "\n",
    "for name, tokenized_data in tokenized_datasets.items():\n",
    "    dataset = T5Dataset(tokenized_data)\n",
    "    loader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "    dataset_loaders[name] = loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c160a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_loader, num_epochs, model, model_name=\"model\", writer=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(Model.parameters(), lr=1e-4)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    Model.train()\n",
    "    global_step = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch} started\")\n",
    "        epoch_loss = 0\n",
    "        for batch in dataset_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            writer.add_scalar(\"Loss/train_batch\", loss.item(), global_step)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            global_step += 1\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(dataset_loader)\n",
    "        writer.add_scalar(\"Loss/train_epoch\", avg_epoch_loss, epoch)\n",
    "\n",
    "        if(epoch % 5 == 0):\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"loss_plots\", exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Curve: {model_name}')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"loss_plots/{model_name}_loss_curve.png\")   # Save figure\n",
    "    plt.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "231e006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff42fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 started\n",
      "Epoch 0, Loss: 7.0842\n",
      "Epoch: 1 started\n",
      "Epoch: 2 started\n",
      "Epoch: 3 started\n",
      "Epoch: 4 started\n",
      "Epoch: 5 started\n",
      "Epoch 5, Loss: 0.5608\n",
      "Epoch: 6 started\n",
      "Epoch: 7 started\n",
      "Epoch: 8 started\n",
      "Epoch: 9 started\n",
      "Epoch: 10 started\n",
      "Epoch 10, Loss: 0.4776\n",
      "Epoch: 11 started\n",
      "Epoch: 12 started\n",
      "Epoch: 13 started\n",
      "Epoch: 14 started\n",
      "Epoch: 15 started\n",
      "Epoch 15, Loss: 0.4959\n",
      "Epoch: 16 started\n",
      "Epoch: 17 started\n",
      "Epoch: 18 started\n",
      "Epoch: 19 started\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2953514/2223461822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Save the model to a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"models/{run_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   3562\u001b[0m                 \u001b[0;31m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m                 \u001b[0;31m# joyfulness), but for now this enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3564\u001b[0;31m                 \u001b[0msafe_save_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3565\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m                 \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/safetensors/torch.py\u001b[0m in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \"\"\"\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mserialize_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_dataloader = dataset_loaders[\"tokenized_WebScrapedData_DiaS_25p\"]\n",
    "run_name = \"WebScrapedData_DiaS_25p\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "trained_model = train(train_dataloader, num_epochs=20, model=model, model_name=run_name, writer=writer)\n",
    "writer.close()\n",
    "\n",
    "# Save the model to a directory\n",
    "trained_model.save_pretrained(f\"models/{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e256798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9460d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/mt5-small\"  # or \"t5-base\", \"t5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56ebf0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Clear freed GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b3d944f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  18835 MiB |  18835 MiB |  23900 MiB |   5065 MiB |\\n|       from large pool |  18762 MiB |  18762 MiB |  23812 MiB |   5050 MiB |\\n|       from small pool |     73 MiB |     74 MiB |     87 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  18835 MiB |  18835 MiB |  23900 MiB |   5065 MiB |\\n|       from large pool |  18762 MiB |  18762 MiB |  23812 MiB |   5050 MiB |\\n|       from small pool |     73 MiB |     74 MiB |     87 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  18818 MiB |  18818 MiB |  23879 MiB |   5061 MiB |\\n|       from large pool |  18745 MiB |  18745 MiB |  23791 MiB |   5046 MiB |\\n|       from small pool |     73 MiB |     74 MiB |     87 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  18988 MiB |  19120 MiB |  19120 MiB | 135168 KiB |\\n|       from large pool |  18892 MiB |  19022 MiB |  19022 MiB | 133120 KiB |\\n|       from small pool |     96 MiB |     98 MiB |     98 MiB |   2048 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 156096 KiB | 217534 KiB |   4704 MiB |   4551 MiB |\\n|       from large pool | 132608 KiB | 194048 KiB |   4629 MiB |   4499 MiB |\\n|       from small pool |  23488 KiB |  25264 KiB |     74 MiB |     51 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     742    |     745    |    1188    |     446    |\\n|       from large pool |     553    |     556    |     875    |     322    |\\n|       from small pool |     189    |     191    |     313    |     124    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     742    |     745    |    1188    |     446    |\\n|       from large pool |     553    |     556    |     875    |     322    |\\n|       from small pool |     189    |     191    |     313    |     124    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     319    |     323    |     323    |       4    |\\n|       from large pool |     271    |     274    |     274    |       3    |\\n|       from small pool |      48    |      49    |      49    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      81    |      84    |     404    |     323    |\\n|       from large pool |      30    |      33    |     299    |     269    |\\n|       from small pool |      51    |      52    |     105    |      54    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)  # Print memory summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cdc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model.save_pretrained(f\"models/{run_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
