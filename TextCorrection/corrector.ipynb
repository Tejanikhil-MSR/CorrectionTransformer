{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c9784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\", src_lang=\"te_IN\", use_fast=False)\n",
    "\n",
    "# vocab = tokenizer.get_vocab()\n",
    "# telugu_tokens = [tok for tok in vocab.keys() if any('\\u0C00' <= ch <= '\\u0C7F' for ch in tok)]\n",
    "\n",
    "# print(telugu_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b189daa",
   "metadata": {},
   "source": [
    "The IndicBART tokenizer is based on SentencePiece, but instead of standard subword units like BPE or WordPiece, it's trained at the Unicode character level, where each token is often a:\n",
    "- Standalone consonant (క, గ, త, etc.)\n",
    "- Vowel sign or diacritic (ా, ి, ీ, etc.)\n",
    "- Word boundary marker (▁ for whitespace)\n",
    "\n",
    "This is intentional for Indic scripts because:\n",
    "\n",
    "- Indic languages are highly agglutinative, and subword segmentation can be noisy.\n",
    "- It's better to model individual aksharas (syllables) or character+diacritic units instead of full words or arbitrary subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1507fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbfe54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/mt5-small\"  # or \"t5-base\", \"t5-large\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name) # loads the tokenizer from hugging face\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name) # load the weights of the model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OriginalText', 'AverageErrorsPerWord_0.15%', '15%_ErrorInducedText',\n",
      "       'AverageErrorsPerWord_0.25%', '25%_ErrorInducedText',\n",
      "       'AverageErrorsPerWord_0.35%', '35%_ErrorInducedText',\n",
      "       'AverageErrorsPerWord_0.5%', '50%_ErrorInducedText'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"./Datasets/SamantarDatasetWithDirectConsonantSubstitutions.csv\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4cf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['OriginalText', '15%_ErrorInducedText']].rename(columns={'OriginalText': 'input', '15%_ErrorInducedText': 'target'})\n",
    "data = data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf284ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    input_enc = tokenizer(text[\"input\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    target_enc = tokenizer(text[\"target\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_data = [preprocess(item) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    # \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.squeeze() for k, v in self.data[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = T5Dataset(tokenized_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c160a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 58.3369\n",
      "Loss: 62.2945\n",
      "Loss: 58.1168\n",
      "Loss: 58.3877\n",
      "Loss: 56.9749\n",
      "Loss: 60.8225\n",
      "Loss: 53.8074\n",
      "Loss: 61.1008\n",
      "Loss: 62.5057\n",
      "Loss: 56.7682\n",
      "Loss: 57.3463\n",
      "Loss: 54.6995\n",
      "Loss: 51.3685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):  # choose your epoch count\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audiofy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
